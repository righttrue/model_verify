{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "# import cartopy.crs as ccrs\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일별 로드 & 특이사항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naebu 20230119 U\n",
      "[19] [16 17]\n",
      "gangbyeon 20230119 U\n",
      "dongbu 20230126 D\n",
      "dongbu 20230127 D\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_vaisala_20230119_U'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 114\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m#===================================================================\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#============================================================================================================================================================\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m#seoul_3_dongbu_lufft_20230127_D.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_vaisala_20230119_U\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m obs_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m obs_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m obs_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    117\u001b[0m file_info \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\demo\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\demo\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\demo\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\demo\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\demo\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_vaisala_20230119_U'"
     ]
    }
   ],
   "source": [
    "#============================================================================================================================================================\n",
    "#seoul_2_naebu_vaisala_20230119_U\n",
    "file = \"DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_2_naebu_vaisala_20230119_U.csv\"\n",
    "obs_df = pd.read_csv(file, low_memory=False)\n",
    "obs_df.columns = obs_df.columns.str.lower()\n",
    "\n",
    "file_info = file.split('_')\n",
    "\n",
    "road_name = file_info[-4]\n",
    "date = file_info[-2]\n",
    "road_dir_1 = file_info[-1]\n",
    "road_dir_1 = road_dir_1.split('.')[0]\n",
    "date = file_info[-2]\n",
    "print(road_name, date, road_dir_1)\n",
    "data_obs_test = obs_df\n",
    "data_obs_test[\"timestamp\"] = pd.to_datetime(data_obs_test[\"timestamp\"])\n",
    "data_obs_test[\"year\"] = data_obs_test[\"timestamp\"].dt.year\n",
    "data_obs_test[\"month\"] = data_obs_test[\"timestamp\"].dt.month\n",
    "data_obs_test[\"day\"] = data_obs_test[\"timestamp\"].dt.day\n",
    "data_obs_test[\"hour\"] = data_obs_test[\"timestamp\"].dt.hour\n",
    "print(data_obs_test[\"day\"].unique(),data_obs_test[\"hour\"].unique())\n",
    "#============================================================================================================================================================\n",
    "#  seoul_1_gangbyeon_vaisala_20230119_U\n",
    "file = \"DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_1_gangbyeon_vaisala_20230119_U.csv\"\n",
    "obs_df = pd.read_csv(file, low_memory=False)\n",
    "obs_df.columns = obs_df.columns.str.lower()\n",
    "\n",
    "file_info = file.split('_')\n",
    "\n",
    "road_name = file_info[-4]\n",
    "date = file_info[-2]\n",
    "road_dir_1 = file_info[-1]\n",
    "road_dir_1 = road_dir_1.split('.')[0]\n",
    "date = file_info[-2]\n",
    "print(road_name, date, road_dir_1)\n",
    "data_obs_test = obs_df\n",
    "data_obs_test[\"timestamp\"] = pd.to_datetime(data_obs_test[\"timestamp\"])\n",
    "data_obs_test[\"year\"] = data_obs_test[\"timestamp\"].dt.year\n",
    "data_obs_test[\"month\"] = data_obs_test[\"timestamp\"].dt.month\n",
    "data_obs_test[\"day\"] = data_obs_test[\"timestamp\"].dt.day\n",
    "data_obs_test[\"hour\"] = data_obs_test[\"timestamp\"].dt.hour\n",
    "\n",
    "\n",
    "# 강병 도로 특이 사항\n",
    "# cond1 = model_road['lon'] >= 126.9061243663102\n",
    "# cond2 = model_road['lat'] <= 37.54867362377485\n",
    "# cond3 = model_road['lon'] <= 126.91043059457456\n",
    "# cond4 = model_road['lat'] >= 37.545676738065886\n",
    "\n",
    "# model_road = model_road[~(cond1 & cond2 & cond3 & cond4)]\n",
    "#============================================================================================================================================================\n",
    "#seoul_3_dongbu_lufft_20230126_D\n",
    "file = \"DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_lufft_20230126_D.csv\"\n",
    "obs_df = pd.read_csv(file, low_memory=False)\n",
    "obs_df.columns = obs_df.columns.str.lower()\n",
    "\n",
    "file_info = file.split('_')\n",
    "\n",
    "road_name = file_info[-4]\n",
    "date = file_info[-2]\n",
    "road_dir_1 = file_info[-1]\n",
    "road_dir_1 = road_dir_1.split('.')[0]\n",
    "date = file_info[-2]\n",
    "print(road_name, date, road_dir_1)\n",
    "data_obs_test = obs_df\n",
    "data_obs_test[\"timestamp\"] = pd.to_datetime(data_obs_test[\"timestamp\"])\n",
    "data_obs_test[\"year\"] = data_obs_test[\"timestamp\"].dt.year\n",
    "data_obs_test[\"month\"] = data_obs_test[\"timestamp\"].dt.month\n",
    "data_obs_test[\"day\"] = data_obs_test[\"timestamp\"].dt.day\n",
    "data_obs_test[\"hour\"] = data_obs_test[\"timestamp\"].dt.hour\n",
    "#===================================================================\n",
    "\n",
    "\n",
    "#============================================================================================================================================================\n",
    "#seoul_3_dongbu_lufft_20230127_D.\n",
    "file = \"DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_lufft_20230127_D.csv\"\n",
    "obs_df = pd.read_csv(file, low_memory=False)\n",
    "obs_df.columns = obs_df.columns.str.lower()\n",
    "\n",
    "file_info = file.split('_')\n",
    "\n",
    "road_name = file_info[-4]\n",
    "date = file_info[-2]\n",
    "road_dir_1 = file_info[-1]\n",
    "road_dir_1 = road_dir_1.split('.')[0]\n",
    "date = file_info[-2]\n",
    "print(road_name, date, road_dir_1)\n",
    "data_obs_test = obs_df\n",
    "data_obs_test[\"timestamp\"] = pd.to_datetime(data_obs_test[\"timestamp\"])\n",
    "data_obs_test[\"year\"] = data_obs_test[\"timestamp\"].dt.year\n",
    "data_obs_test[\"month\"] = data_obs_test[\"timestamp\"].dt.month\n",
    "data_obs_test[\"day\"] = data_obs_test[\"timestamp\"].dt.day\n",
    "data_obs_test[\"hour\"] = data_obs_test[\"timestamp\"].dt.hour\n",
    "\n",
    "\n",
    "# 동부 도로 특이 사항\n",
    "cond1 = model_road['lon'] >= 127.0528247\n",
    "cond2 = model_road['lat'] >= 37.6836121\n",
    "\n",
    "\n",
    "cond3 = model_road['lon'] >=127.0308726\n",
    "cond4 = model_road['lon'] <= 127.0339521\n",
    "\n",
    "cond5 = model_road['lat'] <= 37.5435276\n",
    "cond6 = model_road['lat'] >= 37.5414698\n",
    "\n",
    "model_road = model_road[~(cond1 & cond2)]\n",
    "model_road = model_road[~(cond3 & cond4 & cond5 & cond6)]\n",
    "#===================================================================\n",
    "\n",
    "#============================================================================================================================================================\n",
    "#seoul_3_dongbu_lufft_20230127_D.\n",
    "file = \"DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_vaisala_20230119_U\"\n",
    "obs_df = pd.read_csv(file, low_memory=False)\n",
    "obs_df.columns = obs_df.columns.str.lower()\n",
    "\n",
    "file_info = file.split('_')\n",
    "\n",
    "road_name = file_info[-4]\n",
    "date = file_info[-2]\n",
    "road_dir_1 = file_info[-1]\n",
    "road_dir_1 = road_dir_1.split('.')[0]\n",
    "date = file_info[-2]\n",
    "print(road_name, date, road_dir_1)\n",
    "data_obs_test = obs_df\n",
    "data_obs_test[\"timestamp\"] = pd.to_datetime(data_obs_test[\"timestamp\"])\n",
    "data_obs_test[\"year\"] = data_obs_test[\"timestamp\"].dt.year\n",
    "data_obs_test[\"month\"] = data_obs_test[\"timestamp\"].dt.month\n",
    "data_obs_test[\"day\"] = data_obs_test[\"timestamp\"].dt.day\n",
    "data_obs_test[\"hour\"] = data_obs_test[\"timestamp\"].dt.hour\n",
    "#==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dongbu 20230126 D\n",
      "[26] [11]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#seoul_2_naebu_vaisala_20230119_U\n",
    "file = \"DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_lufft_20230126_D.csv\"\n",
    "obs_df = pd.read_csv(file, low_memory=False)\n",
    "obs_df.columns = obs_df.columns.str.lower()\n",
    "\n",
    "file_info = file.split('_')\n",
    "\n",
    "road_name = file_info[-4]\n",
    "date = file_info[-2]\n",
    "road_dir_1 = file_info[-1]\n",
    "road_dir_1 = road_dir_1.split('.')[0]\n",
    "date = file_info[-2]\n",
    "print(road_name, date, road_dir_1)\n",
    "data_obs_test = obs_df\n",
    "data_obs_test[\"timestamp\"] = pd.to_datetime(data_obs_test[\"timestamp\"])\n",
    "data_obs_test[\"year\"] = data_obs_test[\"timestamp\"].dt.year\n",
    "data_obs_test[\"month\"] = data_obs_test[\"timestamp\"].dt.month\n",
    "data_obs_test[\"day\"] = data_obs_test[\"timestamp\"].dt.day\n",
    "data_obs_test[\"hour\"] = data_obs_test[\"timestamp\"].dt.hour\n",
    "print(data_obs_test[\"day\"].unique(),data_obs_test[\"hour\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 바이살라 _ 중복 해결할 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dongbu 20230119 U\n"
     ]
    }
   ],
   "source": [
    "file = \"DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_vaisala_20230119_U.csv\"\n",
    "obs_df = pd.read_csv(file, low_memory=False)\n",
    "obs_df.columns = obs_df.columns.str.lower()\n",
    "\n",
    "file_info = file.split('_')\n",
    "\n",
    "road_name = file_info[-4]\n",
    "date = file_info[-2]\n",
    "road_dir_1 = file_info[-1]\n",
    "road_dir_1 = road_dir_1.split('.')[0]\n",
    "date = file_info[-2]\n",
    "print(road_name, date, road_dir_1)\n",
    "data_obs_test = obs_df\n",
    "data_obs_test[\"timestamp\"] = pd.to_datetime(data_obs_test[\"timestamp\"])\n",
    "data_obs_test[\"year\"] = data_obs_test[\"timestamp\"].dt.year\n",
    "data_obs_test[\"month\"] = data_obs_test[\"timestamp\"].dt.month\n",
    "data_obs_test[\"day\"] = data_obs_test[\"timestamp\"].dt.day\n",
    "data_obs_test[\"hour\"] = data_obs_test[\"timestamp\"].dt.hour\n",
    "\n",
    "\n",
    "            \n",
    "# 찾은 CSV 파일들을 출력\n",
    "sites = ['jr','mg','org','ss','yc']\n",
    "for site in sites:\n",
    "    jr_model_file = f\"C:/Users/user/Desktop/모델검증/DATA/MODEL/seoul/{site}/202301/18/KMA_HUFS_ROAD-P_seoul_20230118150000.csv\"\n",
    "    model_road = pd.read_csv(jr_model_file)\n",
    "    cond1 = model_road['road_name'] == road_name\n",
    "    cond2 = model_road['direction'].str.startswith(road_dir_1)\n",
    "    model_road = model_road[cond1&cond2]\n",
    "    #site_list.append(model_road)\n",
    "\n",
    "    model_road['date_time'] = pd.to_datetime(model_road['date_time'])\n",
    "    model_road['hour'] = model_road['date_time'].dt.hour\n",
    "    model_road['day'] = model_road['date_time'].dt.day\n",
    "    # cond1 = model_road['lon'] >= 126.9061243663102\n",
    "    # cond2 = model_road['lat'] <= 37.54867362377485\n",
    "    # cond3 = model_road['lon'] <= 126.91043059457456\n",
    "    # cond4 = model_road['lat'] >= 37.545676738065886\n",
    "\n",
    "    # model_road = model_road[~(cond1 & cond2 & cond3 & cond4)]\n",
    "    model_1 = model_road\n",
    "    \n",
    "\n",
    "    model_data_df = model_1.copy() # 해당 조건의 모델 데이터 / 재사용을 위해 복사본 사용\n",
    "    obs_date_df = data_obs_test.copy() # 해당 조건의 관측 데이터 / 재사용을 위해 복사본 사용\n",
    "\n",
    "    #KDTree 를 통한 근접 점 찾기\n",
    "    road_df = pd.DataFrame()\n",
    "    # 첫 번째 줄의 위경도 데이터\n",
    "    observ_line = np.array(list(zip(obs_date_df['longitude'], obs_date_df['latitude'])))\n",
    "\n",
    "    # 두 번째 줄의 위경도 데이터\n",
    "    model_line = np.array(list(zip(model_data_df['lon'], model_data_df['lat'])))\n",
    "\n",
    "    # KDTree 객체 생성\n",
    "    tree = KDTree(observ_line)\n",
    "\n",
    "    # 각 점마다 가장 가까운 점을 찾아 매칭\n",
    "    matched_points = []\n",
    "    for point in model_line:\n",
    "        _, index = tree.query([point], k=1)  # k=1로 설정하여 가장 가까운 점 하나만 선택\n",
    "        matched_points.append(observ_line[index[0]])\n",
    "\n",
    "    matched_lon = [point[0][0] for point in matched_points]\n",
    "    matched_lat = [point[0][1] for point in matched_points]\n",
    "\n",
    "    df = pd.DataFrame({'lon': model_line[:, 0], 'lat': model_line[:, 1],\n",
    "                        'longitude': matched_lon, 'latitude': matched_lat})\n",
    "\n",
    "    model_date_2 = model_data_df.copy()\n",
    "    \n",
    "    total_df = pd.merge(df,data_obs_test,on=['longitude','latitude'])\n",
    "    #print(total_df)\n",
    "\n",
    "    total_df = pd.merge(total_df,model_date_2,on=['lon','lat','hour','day'])\n",
    "    total_df = total_df.drop_duplicates()\n",
    "    road_df = pd.concat([road_df,total_df])\n",
    "\n",
    "    road_df.to_csv(f\"C:/Users/user/Desktop/모델검증/DATA/{road_name}{date}{road_dir_1}_{site}.csv\", index=False)\n",
    "    del road_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 루프트 _ 중복 문제 해결 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dongbu 20230119 U\n",
      "[19] [12 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12568\\1477104878.py:32: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  data_obs_test = data_obs_test[~(cond3 & cond4 & cond5 & cond6)]\n"
     ]
    }
   ],
   "source": [
    "file = \"DATA/20230705_스시2_서울&시흥_1월관측_후처리자료/seoul_3_dongbu_vaisala_20230119_U.csv\"\n",
    "obs_df = pd.read_csv(file, low_memory=False)\n",
    "obs_df.columns = obs_df.columns.str.lower()\n",
    "\n",
    "file_info = file.split('_')\n",
    "\n",
    "road_name = file_info[-4]\n",
    "date = file_info[-2]\n",
    "road_dir_1 = file_info[-1]\n",
    "road_dir_1 = road_dir_1.split('.')[0]\n",
    "date = file_info[-2]\n",
    "print(road_name, date, road_dir_1)\n",
    "data_obs_test = obs_df\n",
    "data_obs_test[\"timestamp\"] = pd.to_datetime(data_obs_test[\"timestamp\"])\n",
    "data_obs_test[\"year\"] = data_obs_test[\"timestamp\"].dt.year\n",
    "data_obs_test[\"month\"] = data_obs_test[\"timestamp\"].dt.month\n",
    "data_obs_test[\"day\"] = data_obs_test[\"timestamp\"].dt.day\n",
    "data_obs_test[\"hour\"] = data_obs_test[\"timestamp\"].dt.hour\n",
    "cond1 = data_obs_test['longitude'] >= 127.0308590093344\n",
    "cond2 = data_obs_test['latitude'] <= 37.543546568354365\n",
    "\n",
    "cond11 = data_obs_test['longitude'] <= 127.06588902545067\n",
    "cond21 = data_obs_test['latitude'] >= 37.531453489606754\n",
    "\n",
    "cond3 = data_obs_test['longitude'] >=127.0308726\n",
    "cond4 = data_obs_test['longitude'] <= 127.0339521\n",
    "\n",
    "cond5 = data_obs_test['latitude'] <= 37.5435276\n",
    "cond6 = data_obs_test['latitude'] >= 37.5414698\n",
    "\n",
    "data_obs_test = data_obs_test[~(cond1 & cond2 & cond11 & cond21)]\n",
    "data_obs_test = data_obs_test[~(cond3 & cond4 & cond5 & cond6)]\n",
    "data_obs_test = data_obs_test\n",
    "    \n",
    "\n",
    "print(data_obs_test[\"day\"].unique(),data_obs_test[\"hour\"].unique())\n",
    "#관측 자료 특이사항 제거\n",
    "\n",
    "            \n",
    "# 찾은 CSV 파일들을 출력\n",
    "sites = ['jr','mg','org','ss','yc']\n",
    "for site in sites:\n",
    "    jr_model_file = f\"C:/Users/user/Desktop/모델검증/DATA/MODEL/seoul/{site}/202301/18/KMA_HUFS_ROAD-P_seoul_20230118150000.csv\"\n",
    "    model_road = pd.read_csv(jr_model_file)\n",
    "    cond1 = model_road['road_name'] == road_name\n",
    "    cond2 = model_road['direction'].str.startswith(road_dir_1)\n",
    "    model_road = model_road[cond1&cond2]\n",
    "    #site_list.append(model_road)\n",
    "\n",
    "    model_road['date_time'] = pd.to_datetime(model_road['date_time'])\n",
    "    model_road['hour'] = model_road['date_time'].dt.hour\n",
    "    model_road['day'] = model_road['date_time'].dt.day\n",
    "    \n",
    "    # 동부 도로 특이 사항\n",
    "    \n",
    "\n",
    "    model_data_df = model_1.copy() # 해당 조건의 모델 데이터 / 재사용을 위해 복사본 사용\n",
    "    obs_date_df = data_obs_test.copy() # 해당 조건의 관측 데이터 / 재사용을 위해 복사본 사용\n",
    "\n",
    "    #KDTree 를 통한 근접 점 찾기\n",
    "    road_df = pd.DataFrame()\n",
    "    # 첫 번째 줄의 위경도 데이터\n",
    "    observ_line = np.array(list(zip(obs_date_df['longitude'], obs_date_df['latitude'])))\n",
    "\n",
    "    # 두 번째 줄의 위경도 데이터\n",
    "    model_line = np.array(list(zip(model_data_df['lon'], model_data_df['lat'])))\n",
    "\n",
    "    # KDTree 객체 생성\n",
    "    tree = KDTree(model_line)\n",
    "\n",
    "    # 각 점마다 가장 가까운 점을 찾아 매칭\n",
    "    matched_points = []\n",
    "    for point in observ_line:\n",
    "        _, index = tree.query([point], k=1)  # k=1로 설정하여 가장 가까운 점 하나만 선택\n",
    "        matched_points.append(model_line[index[0]])\n",
    "\n",
    "    matched_lon = [point[0][0] for point in matched_points]\n",
    "    matched_lat = [point[0][1] for point in matched_points]\n",
    "\n",
    "    df = pd.DataFrame({'longitude': observ_line[:, 0], 'latitude': observ_line[:, 1],\n",
    "                        'lon': matched_lon, 'lat': matched_lat})\n",
    "\n",
    "    model_date_2 = model_data_df.copy()\n",
    "    \n",
    "    total_df = pd.merge(df,data_obs_test,on=['longitude','latitude'])\n",
    "    #print(total_df)\n",
    "\n",
    "    total_df = pd.merge(total_df,model_date_2,on=['lon','lat','hour','day'])\n",
    "    total_df = total_df.drop_duplicates()\n",
    "    road_df = pd.concat([road_df,total_df])\n",
    "    road_df = road_df.drop_duplicates(subset=[\"lon\",\"lat\"])\n",
    "    road_df.to_csv(f\"C:/Users/user/Desktop/모델검증/DATA/{road_name}{date}{road_dir_1}_{site}.csv\", index=False)\n",
    "    del road_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE 계산 _ 폴더별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seobu20230120D_jr.csv 3.2020765577346437\n",
      "jr 3.2020765577346437\n",
      "seobu20230120D_mg.csv 3.866958824738542\n",
      "mg 3.866958824738542\n",
      "seobu20230120D_org.csv 4.070856791474281\n",
      "org 4.070856791474281\n",
      "seobu20230120D_ss.csv 1.9261373363001337\n",
      "ss 1.9261373363001337\n",
      "seobu20230120D_yc.csv 1.673166860645253\n",
      "yc 1.673166860645253\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a list of CSV file paths named csv_files\n",
    "def cal_rmse_df(site):\n",
    "    csv_files = glob.glob('C:/Users/user/Desktop/모델검증/DATA/veri/seobu/' + '*.csv')\n",
    "    \n",
    "    df_list = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)  # Use csv_file instead of csv_files\n",
    "        if site in csv_file:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # Check if either 'road temperature100 [°c] cur' or 'surface_temperature' is present\n",
    "            if 'road temperature100 [°c] cur' in df.columns:\n",
    "                # Rename 'road temperature100 [°c] cur' to 'surface_temperature'\n",
    "                df.rename(columns={'road temperature100 [°c] cur': 'surface_temperature'}, inplace=True)\n",
    "            elif 'surface_temperature' in df.columns:\n",
    "                # Do nothing, as the desired column name is already present\n",
    "                pass\n",
    "            else:\n",
    "                # Handle the case when neither column is present if needed\n",
    "                \n",
    "                pass\n",
    "            df_mse = mean_squared_error(df['road_temp'], df['surface_temperature'])\n",
    "            df_rmse = np.sqrt(df_mse)\n",
    "            print(csv_file.split(\"\\\\\")[-1],end=\" \")\n",
    "            print(df_rmse)\n",
    "            df_list.append(df)\n",
    "\n",
    "    # Concatenate the list of dataframes into a single dataframe\n",
    "    dfs = pd.concat(df_list, ignore_index=True)  # ignore_index=True resets the index\n",
    "    mse = mean_squared_error(dfs['road_temp'], dfs['surface_temperature'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(site,rmse)\n",
    "    \n",
    "    \n",
    "models  = [\"jr\",\"mg\",\"org\",\"ss\",\"yc\"]\n",
    "for model in models:\n",
    "    cal_rmse_df(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dongbu20230119U_jr.csv 2.154183163847431\n",
      "jr 2.154183163847431\n",
      "dongbu20230119U_mg.csv 2.154183163847431\n",
      "mg 2.154183163847431\n",
      "dongbu20230119U_org.csv 2.154183163847431\n",
      "org 2.154183163847431\n",
      "dongbu20230119U_ss.csv 2.154183163847431\n",
      "ss 2.154183163847431\n",
      "dongbu20230119U_yc.csv 2.154183163847431\n",
      "yc 2.154183163847431\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a list of CSV file paths named csv_files\n",
    "def cal_rmse_df(site):\n",
    "    csv_files = glob.glob('C:/Users/user/Desktop/모델검증/DATA/veri/dongbu/19/' + '*.csv')\n",
    "    \n",
    "    df_list = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)  # Use csv_file instead of csv_files\n",
    "        if site in csv_file:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df = df[df['hour'] == 13]\n",
    "            # Check if either 'road temperature100 [°c] cur' or 'surface_temperature' is present\n",
    "            if 'road temperature100 [°c] cur' in df.columns:\n",
    "                # Rename 'road temperature100 [°c] cur' to 'surface_temperature'\n",
    "                df.rename(columns={'road temperature100 [°c] cur': 'surface_temperature'}, inplace=True)\n",
    "            elif 'surface_temperature' in df.columns:\n",
    "                # Do nothing, as the desired column name is already present\n",
    "                pass\n",
    "            else:\n",
    "                # Handle the case when neither column is present if needed\n",
    "                \n",
    "                pass\n",
    "            df_mse = mean_squared_error(df['road_temp'], df['surface_temperature'])\n",
    "            df_rmse = np.sqrt(df_mse)\n",
    "            print(csv_file.split(\"\\\\\")[-1],end=\" \")\n",
    "            print(df_rmse)\n",
    "            df_list.append(df)\n",
    "\n",
    "    # Concatenate the list of dataframes into a single dataframe\n",
    "    dfs = pd.concat(df_list, ignore_index=True)  # ignore_index=True resets the index\n",
    "    mse = mean_squared_error(dfs['road_temp'], dfs['surface_temperature'])\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(site,rmse)\n",
    "    \n",
    "    \n",
    "models  = [\"jr\",\"mg\",\"org\",\"ss\",\"yc\"]\n",
    "for model in models:\n",
    "    cal_rmse_df(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
